{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31b14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2998972453777236\n",
      "=== epoch:1, train acc:0.231, test acc:0.236 ===\n",
      "train loss:2.295721821361099\n",
      "train loss:2.288797818059546\n",
      "train loss:2.2841339059756702\n",
      "train loss:2.27822485853553\n",
      "train loss:2.2654685828974954\n",
      "train loss:2.254407780657926\n",
      "train loss:2.21511196094543\n",
      "train loss:2.1833696380173278\n",
      "train loss:2.168737485157131\n",
      "train loss:2.131537804891339\n",
      "train loss:2.1498446396043263\n",
      "train loss:2.0676024018177785\n",
      "train loss:2.00090114937646\n",
      "train loss:1.9561788229597452\n",
      "train loss:1.8821448089506156\n",
      "train loss:1.8058231722747156\n",
      "train loss:1.8361972511286495\n",
      "train loss:1.6240014376661622\n",
      "train loss:1.5942134177036036\n",
      "train loss:1.482739976682936\n",
      "train loss:1.3107105583848782\n",
      "train loss:1.412410901215471\n",
      "train loss:1.393048122557562\n",
      "train loss:1.2309868660497612\n",
      "train loss:1.035170638462059\n",
      "train loss:1.0425710252773392\n",
      "train loss:0.8970823858054569\n",
      "train loss:0.8382165709730691\n",
      "train loss:0.8678618106363973\n",
      "train loss:0.8887877529012331\n",
      "train loss:0.8307713063549584\n",
      "train loss:0.6764925516209511\n",
      "train loss:0.7320237483199844\n",
      "train loss:0.5813300357168228\n",
      "train loss:0.5954408785613721\n",
      "train loss:0.5876015237018936\n",
      "train loss:0.7100260937790405\n",
      "train loss:0.6821419012441524\n",
      "train loss:0.8455022149695429\n",
      "train loss:0.42184531285834287\n",
      "train loss:0.9155940967557329\n",
      "train loss:0.6495304953337159\n",
      "train loss:0.5576716989574292\n",
      "train loss:0.7500396761757354\n",
      "train loss:0.5781258598867851\n",
      "train loss:0.5645826043118088\n",
      "train loss:0.43885959437197725\n",
      "train loss:0.3089960807098071\n",
      "train loss:0.6610091535602003\n",
      "train loss:0.676997192803789\n",
      "=== epoch:2, train acc:0.83, test acc:0.812 ===\n",
      "train loss:0.5866926194674182\n",
      "train loss:0.48354758370115714\n",
      "train loss:0.5369175526312173\n",
      "train loss:0.5955024826743811\n",
      "train loss:0.551076247993209\n",
      "train loss:0.37480928104601297\n",
      "train loss:0.5050715927019462\n",
      "train loss:0.4340796786140633\n",
      "train loss:0.5008122373489772\n",
      "train loss:0.49102893078791565\n",
      "train loss:0.4124061137492882\n",
      "train loss:0.5531937522505472\n",
      "train loss:0.40975425041347746\n",
      "train loss:0.4458389461704616\n",
      "train loss:0.41596780939410377\n",
      "train loss:0.47367420643496666\n",
      "train loss:0.39422905764879945\n",
      "train loss:0.4994055117311968\n",
      "train loss:0.40425788839531274\n",
      "train loss:0.3716450983032333\n",
      "train loss:0.4314925675681429\n",
      "train loss:0.3853702293417895\n",
      "train loss:0.3701439029568148\n",
      "train loss:0.3042265740636428\n",
      "train loss:0.3848605719955871\n",
      "train loss:0.30516178375506053\n",
      "train loss:0.3662143420694766\n",
      "train loss:0.42073337459657295\n",
      "train loss:0.37460016828053666\n",
      "train loss:0.3635658234931098\n",
      "train loss:0.29748380822262344\n",
      "train loss:0.45254807816027715\n",
      "train loss:0.49769483177768165\n",
      "train loss:0.2968645310854414\n",
      "train loss:0.2969609194101691\n",
      "train loss:0.5272057591939102\n",
      "train loss:0.46550159541851654\n",
      "train loss:0.35155213234758653\n",
      "train loss:0.24334878234396576\n",
      "train loss:0.2979259238218249\n",
      "train loss:0.24753760166079097\n",
      "train loss:0.27754363876166177\n",
      "train loss:0.3397225229221729\n",
      "train loss:0.44824234604331714\n",
      "train loss:0.2880013314356626\n",
      "train loss:0.35518353631732646\n",
      "train loss:0.3508431081070947\n",
      "train loss:0.22182316143083886\n",
      "train loss:0.2909291750123166\n",
      "train loss:0.24697761432243082\n",
      "=== epoch:3, train acc:0.879, test acc:0.855 ===\n",
      "train loss:0.2104473327146279\n",
      "train loss:0.2267140179597399\n",
      "train loss:0.3459636842276459\n",
      "train loss:0.26634505239289546\n",
      "train loss:0.3526509766608803\n",
      "train loss:0.37887342626158643\n",
      "train loss:0.36916446681101367\n",
      "train loss:0.29692755745696275\n",
      "train loss:0.3606431825861178\n",
      "train loss:0.3725499167276334\n",
      "train loss:0.4497675866718793\n",
      "train loss:0.19500461557992732\n",
      "train loss:0.32701207081734573\n",
      "train loss:0.3406195355204548\n",
      "train loss:0.2967010825387845\n",
      "train loss:0.3921215793163669\n",
      "train loss:0.36155704075646683\n",
      "train loss:0.26317588602499525\n",
      "train loss:0.19223938016805678\n",
      "train loss:0.19417394853739917\n",
      "train loss:0.39142151243787643\n",
      "train loss:0.2903117052406735\n",
      "train loss:0.1703622120049967\n",
      "train loss:0.26011869579256425\n",
      "train loss:0.3202317915340679\n",
      "train loss:0.30634644562977625\n",
      "train loss:0.2945006645617219\n",
      "train loss:0.3457024774294101\n",
      "train loss:0.25906196696664546\n",
      "train loss:0.26371285598768307\n",
      "train loss:0.24820172390156872\n",
      "train loss:0.18506835798149995\n",
      "train loss:0.24213111808171867\n",
      "train loss:0.3078902659500305\n",
      "train loss:0.2570679032883506\n",
      "train loss:0.3751612139897743\n",
      "train loss:0.17046655427041887\n",
      "train loss:0.2767420311823528\n",
      "train loss:0.3221887869921162\n",
      "train loss:0.15030951570608317\n",
      "train loss:0.4544311282846902\n",
      "train loss:0.2657180936971886\n",
      "train loss:0.2630976997998023\n",
      "train loss:0.27269565411023683\n",
      "train loss:0.23994904038972398\n",
      "train loss:0.3723428159012921\n",
      "train loss:0.2745381942085678\n",
      "train loss:0.1501162104172783\n",
      "train loss:0.28463401087258794\n",
      "train loss:0.3658481311848022\n",
      "=== epoch:4, train acc:0.903, test acc:0.879 ===\n",
      "train loss:0.3778350705102986\n",
      "train loss:0.2655439821719925\n",
      "train loss:0.20844206179160235\n",
      "train loss:0.2832652135287503\n",
      "train loss:0.2606367133006726\n",
      "train loss:0.2919890088126184\n",
      "train loss:0.3007390052833128\n",
      "train loss:0.16908336881178554\n",
      "train loss:0.2996213344783028\n",
      "train loss:0.3350839113157874\n",
      "train loss:0.25825946227846297\n",
      "train loss:0.30249420816815276\n",
      "train loss:0.21824251019842042\n",
      "train loss:0.21044651651061322\n",
      "train loss:0.19518154247989233\n",
      "train loss:0.27020926783901417\n",
      "train loss:0.21018320593917772\n",
      "train loss:0.18839892831475769\n",
      "train loss:0.15188824548936458\n",
      "train loss:0.2570602157341925\n",
      "train loss:0.2600577790019733\n",
      "train loss:0.2783269315156989\n",
      "train loss:0.3330979402120113\n",
      "train loss:0.2563060926752122\n",
      "train loss:0.5437106164827815\n",
      "train loss:0.22462095143440788\n",
      "train loss:0.1791177186332254\n",
      "train loss:0.320870392283842\n",
      "train loss:0.24823151702851345\n",
      "train loss:0.16513603083738637\n",
      "train loss:0.20103627167369612\n",
      "train loss:0.1975601924345992\n",
      "train loss:0.19753542724048942\n",
      "train loss:0.1720574693922511\n",
      "train loss:0.3128828695733825\n",
      "train loss:0.2839549582313135\n",
      "train loss:0.2409475431147308\n",
      "train loss:0.3154181108012939\n",
      "train loss:0.18754719550695476\n",
      "train loss:0.16479195502095564\n",
      "train loss:0.1879996185552947\n",
      "train loss:0.31978133712152396\n",
      "train loss:0.20785998528489683\n",
      "train loss:0.15727133934875565\n",
      "train loss:0.23888312547885357\n",
      "train loss:0.26231937449906223\n",
      "train loss:0.2187855819788022\n",
      "train loss:0.21449900931428745\n",
      "train loss:0.15239385410005515\n",
      "train loss:0.1607134391934807\n",
      "=== epoch:5, train acc:0.911, test acc:0.897 ===\n",
      "train loss:0.2840499418596136\n",
      "train loss:0.3012803666615602\n",
      "train loss:0.19623624804647105\n",
      "train loss:0.2592180289740677\n",
      "train loss:0.3158027808798588\n",
      "train loss:0.32040571274137525\n",
      "train loss:0.17820996872962744\n",
      "train loss:0.2754377211097262\n",
      "train loss:0.243584468256012\n",
      "train loss:0.340804505217349\n",
      "train loss:0.2532835700294585\n",
      "train loss:0.1388826090700791\n",
      "train loss:0.17412750901519583\n",
      "train loss:0.2474685139285005\n",
      "train loss:0.14930096934115814\n",
      "train loss:0.16859785895196353\n",
      "train loss:0.12953458815405072\n",
      "train loss:0.2954613709525601\n",
      "train loss:0.14708093118306403\n",
      "train loss:0.20487399133169934\n",
      "train loss:0.13270649695101974\n",
      "train loss:0.13266821064785708\n",
      "train loss:0.18035642235136823\n",
      "train loss:0.2336559303084298\n",
      "train loss:0.16662544778088517\n",
      "train loss:0.28659564527709125\n",
      "train loss:0.22386696457449878\n",
      "train loss:0.1852247178176279\n",
      "train loss:0.16302084291214985\n",
      "train loss:0.18636098052038605\n",
      "train loss:0.451505490884585\n",
      "train loss:0.16260310510245385\n",
      "train loss:0.4263922808723104\n",
      "train loss:0.26771560049594906\n",
      "train loss:0.24339227135228278\n",
      "train loss:0.15037472364921967\n",
      "train loss:0.2587355025445441\n",
      "train loss:0.31641312552803247\n",
      "train loss:0.2708575842888296\n",
      "train loss:0.3378400954112491\n",
      "train loss:0.2856198123319538\n",
      "train loss:0.197585284645219\n",
      "train loss:0.20868982177319104\n",
      "train loss:0.1778164859753906\n",
      "train loss:0.174327839244098\n",
      "train loss:0.1833006739589588\n",
      "train loss:0.19535856840809182\n",
      "train loss:0.14906840049655268\n",
      "train loss:0.2279949233077575\n",
      "train loss:0.1966831434695125\n",
      "=== epoch:6, train acc:0.936, test acc:0.9 ===\n",
      "train loss:0.1529343062857253\n",
      "train loss:0.21753843422099994\n",
      "train loss:0.27259121545191467\n",
      "train loss:0.2654916894887017\n",
      "train loss:0.16086166555967352\n",
      "train loss:0.33410851090527854\n",
      "train loss:0.21452574531542998\n",
      "train loss:0.23837436215848182\n",
      "train loss:0.1664874604583834\n",
      "train loss:0.17211641087870821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.17911965133376356\n",
      "train loss:0.16449666979399644\n",
      "train loss:0.17754374900412143\n",
      "train loss:0.15226978381163947\n",
      "train loss:0.15777115004269948\n",
      "train loss:0.3016738467541814\n",
      "train loss:0.12276027830322692\n",
      "train loss:0.1657865689700952\n",
      "train loss:0.11852511716131929\n",
      "train loss:0.1435273385484474\n",
      "train loss:0.1720000905946132\n",
      "train loss:0.1162212284689653\n",
      "train loss:0.17980367391084667\n",
      "train loss:0.1874433484931004\n",
      "train loss:0.13710831938378618\n",
      "train loss:0.20320226145708276\n",
      "train loss:0.22656464317035646\n",
      "train loss:0.11600708628225544\n",
      "train loss:0.13312522555207063\n",
      "train loss:0.11775014191137143\n",
      "train loss:0.11302190815712593\n",
      "train loss:0.2015829675568776\n",
      "train loss:0.199389357295707\n",
      "train loss:0.14051953284921084\n",
      "train loss:0.19193645043148724\n",
      "train loss:0.18315021193309156\n",
      "train loss:0.13297242167011303\n",
      "train loss:0.07140574099394126\n",
      "train loss:0.15834215954034334\n",
      "train loss:0.16538371490409318\n",
      "train loss:0.1579172814567766\n",
      "train loss:0.10891936823044583\n",
      "train loss:0.09234667245407945\n",
      "train loss:0.22762674352899775\n",
      "train loss:0.18541028916700242\n",
      "train loss:0.20509475465863486\n",
      "train loss:0.1153810415672039\n",
      "train loss:0.17070368066768485\n",
      "train loss:0.12074017511020103\n",
      "train loss:0.24499670930750064\n",
      "=== epoch:7, train acc:0.937, test acc:0.917 ===\n",
      "train loss:0.20173980961383844\n",
      "train loss:0.12491338155359126\n",
      "train loss:0.11601889259162947\n",
      "train loss:0.17625312799413223\n",
      "train loss:0.15642689525019815\n",
      "train loss:0.19385190889908863\n",
      "train loss:0.0954035535231857\n",
      "train loss:0.16806557929275343\n",
      "train loss:0.10503723192590343\n",
      "train loss:0.2827391563458656\n",
      "train loss:0.1875002639872395\n",
      "train loss:0.26151260015753985\n",
      "train loss:0.13554065518495415\n",
      "train loss:0.22268158696610788\n",
      "train loss:0.280564958179079\n",
      "train loss:0.0942010512554728\n",
      "train loss:0.16839885598700038\n",
      "train loss:0.17458094465303714\n",
      "train loss:0.14693684166324833\n",
      "train loss:0.08905909995785777\n",
      "train loss:0.16153731806387378\n",
      "train loss:0.134271071525521\n",
      "train loss:0.17072691066131507\n",
      "train loss:0.07800908819259143\n",
      "train loss:0.19841564831272063\n",
      "train loss:0.0999886186798232\n",
      "train loss:0.20910235580865819\n",
      "train loss:0.11490299853383626\n",
      "train loss:0.11110621700856876\n",
      "train loss:0.10003062590338964\n",
      "train loss:0.11847004499108377\n",
      "train loss:0.13806091776392707\n",
      "train loss:0.09081699698599598\n",
      "train loss:0.12286581918461092\n",
      "train loss:0.16282771236985621\n",
      "train loss:0.20260807826030525\n",
      "train loss:0.11654426097108869\n",
      "train loss:0.11962770957628162\n",
      "train loss:0.14529075067128866\n",
      "train loss:0.16062164401610293\n",
      "train loss:0.09251105424176038\n",
      "train loss:0.1965830345058321\n",
      "train loss:0.210915590530914\n",
      "train loss:0.11865213604065718\n",
      "train loss:0.20646138847933163\n",
      "train loss:0.12778038618705043\n",
      "train loss:0.0646868759641299\n",
      "train loss:0.14115258116728746\n",
      "train loss:0.2356737116790462\n",
      "train loss:0.12195779661938788\n",
      "=== epoch:8, train acc:0.941, test acc:0.92 ===\n",
      "train loss:0.20091720287693582\n",
      "train loss:0.17193203782571664\n",
      "train loss:0.11011330083613341\n",
      "train loss:0.1648326381044777\n",
      "train loss:0.09614438401061878\n",
      "train loss:0.24738012562525338\n",
      "train loss:0.09769323334371613\n",
      "train loss:0.21757136470228985\n",
      "train loss:0.06149849639383844\n",
      "train loss:0.10479033976241635\n",
      "train loss:0.1655250807815127\n",
      "train loss:0.1486039203200234\n",
      "train loss:0.11084594590776042\n",
      "train loss:0.17027296638539344\n",
      "train loss:0.09911122944491452\n",
      "train loss:0.10935365094321073\n",
      "train loss:0.20146543735901173\n",
      "train loss:0.1490988985874275\n",
      "train loss:0.06445293611831839\n",
      "train loss:0.06068141748519947\n",
      "train loss:0.046408518315901176\n",
      "train loss:0.11481078282075406\n",
      "train loss:0.060948209518454005\n",
      "train loss:0.10441862729641752\n",
      "train loss:0.10538708892120113\n",
      "train loss:0.22711882383404125\n",
      "train loss:0.06926626704630601\n",
      "train loss:0.13036965745109017\n",
      "train loss:0.08887719166148733\n",
      "train loss:0.13139513475714754\n",
      "train loss:0.1503777280825192\n",
      "train loss:0.0899568875460191\n",
      "train loss:0.09418978948913455\n",
      "train loss:0.09660875048195822\n",
      "train loss:0.14694193358577773\n",
      "train loss:0.15257143328760575\n",
      "train loss:0.1693014744734996\n",
      "train loss:0.068135303446926\n",
      "train loss:0.13559870540718968\n",
      "train loss:0.11686238298954935\n",
      "train loss:0.20742772464072257\n",
      "train loss:0.19407700230747063\n",
      "train loss:0.10636367001146668\n",
      "train loss:0.06446010213147368\n",
      "train loss:0.09152556518758233\n",
      "train loss:0.08796845206917965\n",
      "train loss:0.10071333787537214\n",
      "train loss:0.07634328493301475\n",
      "train loss:0.14427700218944028\n",
      "train loss:0.11178333285617369\n",
      "=== epoch:9, train acc:0.957, test acc:0.932 ===\n",
      "train loss:0.07953699333846038\n",
      "train loss:0.07671349650734709\n",
      "train loss:0.1019800895094898\n",
      "train loss:0.08105558730293443\n",
      "train loss:0.06569139697823027\n",
      "train loss:0.16529161268658385\n",
      "train loss:0.05792595656937366\n",
      "train loss:0.08815072364224574\n",
      "train loss:0.20604147020299313\n",
      "train loss:0.050296912369779435\n",
      "train loss:0.12032133425315099\n",
      "train loss:0.1908170950410765\n",
      "train loss:0.061988105001777614\n",
      "train loss:0.09947788710395913\n",
      "train loss:0.15972089663998043\n",
      "train loss:0.09033266850460847\n",
      "train loss:0.26178340995233873\n",
      "train loss:0.08725693637239126\n",
      "train loss:0.16933720740331562\n",
      "train loss:0.11274722773684877\n",
      "train loss:0.09120866882603403\n",
      "train loss:0.0952192555093313\n",
      "train loss:0.09903121889644957\n",
      "train loss:0.07697677573410734\n",
      "train loss:0.06022383248061468\n",
      "train loss:0.13546532510446213\n",
      "train loss:0.11180745079604101\n",
      "train loss:0.09627620046771543\n",
      "train loss:0.11396786157857398\n",
      "train loss:0.0817548858338217\n",
      "train loss:0.13658267914704186\n",
      "train loss:0.14828909627024053\n",
      "train loss:0.19111198712331887\n",
      "train loss:0.07478064798672956\n",
      "train loss:0.08460544726764685\n",
      "train loss:0.09155712527895267\n",
      "train loss:0.147134814643152\n",
      "train loss:0.09785145989192498\n",
      "train loss:0.08036953002815127\n",
      "train loss:0.1004837687279875\n",
      "train loss:0.09639851627135826\n",
      "train loss:0.08560219529518408\n",
      "train loss:0.11974251622596277\n",
      "train loss:0.06762250993731145\n",
      "train loss:0.03417165439336231\n",
      "train loss:0.061195473897478016\n",
      "train loss:0.10429869618338115\n",
      "train loss:0.15776931073765543\n",
      "train loss:0.09323820646737593\n",
      "train loss:0.15675056409042418\n",
      "=== epoch:10, train acc:0.96, test acc:0.938 ===\n",
      "train loss:0.20060387771497465\n",
      "train loss:0.09441364785851197\n",
      "train loss:0.09766048321121634\n",
      "train loss:0.11569931069124056\n",
      "train loss:0.09745979913085495\n",
      "train loss:0.04894957382249217\n",
      "train loss:0.15534388364445392\n",
      "train loss:0.07455429339716059\n",
      "train loss:0.07465658844726143\n",
      "train loss:0.11408072557092\n",
      "train loss:0.2088214786898618\n",
      "train loss:0.07312132786359041\n",
      "train loss:0.08415952753640771\n",
      "train loss:0.06468805158560746\n",
      "train loss:0.05678270286318158\n",
      "train loss:0.06084246348510632\n",
      "train loss:0.0726827194456717\n",
      "train loss:0.143789469967781\n",
      "train loss:0.04310033135829906\n",
      "train loss:0.21774207242050886\n",
      "train loss:0.14373163142025558\n",
      "train loss:0.039621674073002044\n",
      "train loss:0.06034677649824545\n",
      "train loss:0.05973320472623804\n",
      "train loss:0.07533114719803899\n",
      "train loss:0.10285953271646857\n",
      "train loss:0.10523424232575267\n",
      "train loss:0.13883587093339833\n",
      "train loss:0.05550222384819099\n",
      "train loss:0.06461621603080653\n",
      "train loss:0.1419537407166313\n",
      "train loss:0.08650044392611267\n",
      "train loss:0.07230566330592397\n",
      "train loss:0.060251703550652326\n",
      "train loss:0.09539312723270658\n",
      "train loss:0.1298898576589875\n",
      "train loss:0.08891813909984167\n",
      "train loss:0.07782807746604463\n",
      "train loss:0.11754954885678645\n",
      "train loss:0.29470345143560084\n",
      "train loss:0.10944524622654932\n",
      "train loss:0.06235005610434342\n",
      "train loss:0.25094398749323454\n",
      "train loss:0.17460744918696242\n",
      "train loss:0.10853422043406359\n",
      "train loss:0.14967465731202145\n",
      "train loss:0.1181038075127745\n",
      "train loss:0.0699634019431723\n",
      "train loss:0.15903707474874987\n",
      "train loss:0.07305709370572394\n",
      "=== epoch:11, train acc:0.964, test acc:0.945 ===\n",
      "train loss:0.08217612333129455\n",
      "train loss:0.09581030946589345\n",
      "train loss:0.12323605330410276\n",
      "train loss:0.09152595789696932\n",
      "train loss:0.12705640656614123\n",
      "train loss:0.07121982068590817\n",
      "train loss:0.16972599206371505\n",
      "train loss:0.09098953420761025\n",
      "train loss:0.0725511680971808\n",
      "train loss:0.12042515393811483\n",
      "train loss:0.12440534585256763\n",
      "train loss:0.12643002919716648\n",
      "train loss:0.1830738173453883\n",
      "train loss:0.166731449603847\n",
      "train loss:0.12477318294440677\n",
      "train loss:0.12051359810219699\n",
      "train loss:0.07501366003810579\n",
      "train loss:0.07761205083427805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0922066591884469\n",
      "train loss:0.08789019879094434\n",
      "train loss:0.06764051123078806\n",
      "train loss:0.08048817871241942\n",
      "train loss:0.08360758809501495\n",
      "train loss:0.060974782995182865\n",
      "train loss:0.18063967817236562\n",
      "train loss:0.12091837963385499\n",
      "train loss:0.06982424192182805\n",
      "train loss:0.06931443688554544\n",
      "train loss:0.04866865617027354\n",
      "train loss:0.08672213577154159\n",
      "train loss:0.18713296700642856\n",
      "train loss:0.12985945465072468\n",
      "train loss:0.05688453948632032\n",
      "train loss:0.11003848266392822\n",
      "train loss:0.10186432280767392\n",
      "train loss:0.16675982910918655\n",
      "train loss:0.03754229290742021\n",
      "train loss:0.1416082929752038\n",
      "train loss:0.1636464995277929\n",
      "train loss:0.05221457067204132\n",
      "train loss:0.054443020922480254\n",
      "train loss:0.04295349970431851\n",
      "train loss:0.08968580408668289\n",
      "train loss:0.13193238613297276\n",
      "train loss:0.11219675948890058\n",
      "train loss:0.05433765374738495\n",
      "train loss:0.059073822233533635\n",
      "train loss:0.07006949567422761\n",
      "train loss:0.06812992480584987\n",
      "train loss:0.08373005355572072\n",
      "=== epoch:12, train acc:0.967, test acc:0.951 ===\n",
      "train loss:0.061401594164269185\n",
      "train loss:0.09724190833865594\n",
      "train loss:0.10771750334944553\n",
      "train loss:0.06965240331065517\n",
      "train loss:0.08597584409818645\n",
      "train loss:0.056619871876448576\n",
      "train loss:0.054899164842150655\n",
      "train loss:0.06834300343754912\n",
      "train loss:0.07318684972125505\n",
      "train loss:0.10804284798631804\n",
      "train loss:0.03580903712586713\n",
      "train loss:0.06710472288636914\n",
      "train loss:0.024547565038243895\n",
      "train loss:0.11705141712453378\n",
      "train loss:0.09855985290579561\n",
      "train loss:0.08393753077717168\n",
      "train loss:0.07679691772717663\n",
      "train loss:0.03263463259931781\n",
      "train loss:0.0183963545622131\n",
      "train loss:0.04549224881367331\n",
      "train loss:0.08902307946331446\n",
      "train loss:0.11792267377271744\n",
      "train loss:0.06848443421407346\n",
      "train loss:0.1226808163230036\n",
      "train loss:0.12868865575019525\n",
      "train loss:0.03340941098954869\n",
      "train loss:0.0295268425100041\n",
      "train loss:0.11294031103217093\n",
      "train loss:0.028317402422383148\n",
      "train loss:0.06582231805780794\n",
      "train loss:0.04815121390342454\n",
      "train loss:0.0737324763832988\n",
      "train loss:0.03910550980429448\n",
      "train loss:0.04752162227180471\n",
      "train loss:0.12970638733982456\n",
      "train loss:0.06515871359596287\n",
      "train loss:0.06459538749623932\n",
      "train loss:0.18686772869241747\n",
      "train loss:0.09173655519037034\n",
      "train loss:0.02905021961310859\n",
      "train loss:0.049069176744003976\n",
      "train loss:0.03317204569502924\n",
      "train loss:0.042545019100058486\n",
      "train loss:0.06580573180260774\n",
      "train loss:0.0777032774874716\n",
      "train loss:0.05636752601291503\n",
      "train loss:0.04230492181856453\n",
      "train loss:0.030777600605280026\n",
      "train loss:0.03329222678379575\n",
      "train loss:0.09190547175207467\n",
      "=== epoch:13, train acc:0.971, test acc:0.948 ===\n",
      "train loss:0.06059929633238948\n",
      "train loss:0.06239567953513415\n",
      "train loss:0.03935058921177848\n",
      "train loss:0.038679015822240224\n",
      "train loss:0.08016049675119037\n",
      "train loss:0.07570584935420596\n",
      "train loss:0.04490793211441453\n",
      "train loss:0.03800216963150368\n",
      "train loss:0.08708712296398848\n",
      "train loss:0.16537674754385867\n",
      "train loss:0.04107081348925191\n",
      "train loss:0.04254299454392142\n",
      "train loss:0.04429835180723192\n",
      "train loss:0.050612885577438475\n",
      "train loss:0.09743460003766971\n",
      "train loss:0.04556626831351793\n",
      "train loss:0.056072028250003444\n",
      "train loss:0.09827708807030962\n",
      "train loss:0.08971928869681133\n",
      "train loss:0.07617038873412031\n",
      "train loss:0.060672278146465024\n",
      "train loss:0.054633659303697464\n",
      "train loss:0.07509410956035459\n",
      "train loss:0.03394148895155194\n",
      "train loss:0.08072006008397324\n",
      "train loss:0.08015227308430543\n",
      "train loss:0.06449765341428897\n",
      "train loss:0.0835551678771667\n",
      "train loss:0.0510099254720081\n",
      "train loss:0.06092281735040255\n",
      "train loss:0.045432468670253565\n",
      "train loss:0.01963683788596288\n",
      "train loss:0.0260125006550994\n",
      "train loss:0.046746456441686235\n",
      "train loss:0.13486901593156872\n",
      "train loss:0.034393895960199775\n",
      "train loss:0.046486220005573654\n",
      "train loss:0.03610698421510589\n",
      "train loss:0.06398673484559189\n",
      "train loss:0.0986721029591076\n",
      "train loss:0.09843319272885304\n",
      "train loss:0.0243067054063434\n",
      "train loss:0.11066831780836911\n",
      "train loss:0.03478593030271294\n",
      "train loss:0.10695095536170975\n",
      "train loss:0.15624853710827863\n",
      "train loss:0.04709135059958047\n",
      "train loss:0.017763758289893624\n",
      "train loss:0.07452436747139009\n",
      "train loss:0.0843949413150864\n",
      "=== epoch:14, train acc:0.973, test acc:0.947 ===\n",
      "train loss:0.14875875995366286\n",
      "train loss:0.09044723070296862\n",
      "train loss:0.05289261993243519\n",
      "train loss:0.043140886958301865\n",
      "train loss:0.023545849910063005\n",
      "train loss:0.07834746308271812\n",
      "train loss:0.06095479522470737\n",
      "train loss:0.0637451685597925\n",
      "train loss:0.08541351419550047\n",
      "train loss:0.04242976733400094\n",
      "train loss:0.057417825837842676\n",
      "train loss:0.06039664440823033\n",
      "train loss:0.04538827105777782\n",
      "train loss:0.07676930172486043\n",
      "train loss:0.04103045491233038\n",
      "train loss:0.04861365833497886\n",
      "train loss:0.0327577550482771\n",
      "train loss:0.0352948772804738\n",
      "train loss:0.07991025682245077\n",
      "train loss:0.14799036924779327\n",
      "train loss:0.034439926629150006\n",
      "train loss:0.05010183193801252\n",
      "train loss:0.04849153068422806\n",
      "train loss:0.024208729713350415\n",
      "train loss:0.031610495755400654\n",
      "train loss:0.05173007018215544\n",
      "train loss:0.05292139776051737\n",
      "train loss:0.027840529525490845\n",
      "train loss:0.13313585449854276\n",
      "train loss:0.03387348734142823\n",
      "train loss:0.0894412132771392\n",
      "train loss:0.1364006324717233\n",
      "train loss:0.018736586530325915\n",
      "train loss:0.053211360916860156\n",
      "train loss:0.12263068792172536\n",
      "train loss:0.022063593616175967\n",
      "train loss:0.04302644104136773\n",
      "train loss:0.06487275064989889\n",
      "train loss:0.033419576574938666\n",
      "train loss:0.1106453533930104\n",
      "train loss:0.0956415000684559\n",
      "train loss:0.0652275623674473\n",
      "train loss:0.0275776855500069\n",
      "train loss:0.06455351218404223\n",
      "train loss:0.04540817664502941\n",
      "train loss:0.0337133411255559\n",
      "train loss:0.10013389461087892\n",
      "train loss:0.04577980162691274\n",
      "train loss:0.050710534666450986\n",
      "train loss:0.05073827820588621\n",
      "=== epoch:15, train acc:0.973, test acc:0.947 ===\n",
      "train loss:0.04884999564824992\n",
      "train loss:0.059203403919781336\n",
      "train loss:0.06376823533083542\n",
      "train loss:0.03804461332316666\n",
      "train loss:0.03842568385051129\n",
      "train loss:0.04978700425888846\n",
      "train loss:0.03907490862350748\n",
      "train loss:0.05836610864470173\n",
      "train loss:0.0500678103456906\n",
      "train loss:0.030457932306991235\n",
      "train loss:0.03554616735390659\n",
      "train loss:0.02700536844367608\n",
      "train loss:0.033671610609154363\n",
      "train loss:0.0427453445375376\n",
      "train loss:0.0535744349865435\n",
      "train loss:0.07709001059168061\n",
      "train loss:0.03318018957346741\n",
      "train loss:0.055016989628228305\n",
      "train loss:0.050807753184839745\n",
      "train loss:0.13121180085819073\n",
      "train loss:0.05174541228912622\n",
      "train loss:0.041690181652822095\n",
      "train loss:0.056555111510088685\n",
      "train loss:0.12832230178493223\n",
      "train loss:0.07169344484486634\n",
      "train loss:0.030621723071586716\n",
      "train loss:0.040507895251373716\n",
      "train loss:0.048392650288670236\n",
      "train loss:0.04115463060308421\n",
      "train loss:0.01644919075774125\n",
      "train loss:0.05190159885030199\n",
      "train loss:0.0211607905118643\n",
      "train loss:0.05632444490967839\n",
      "train loss:0.049795523432852436\n",
      "train loss:0.045437442549786454\n",
      "train loss:0.039484184252229056\n",
      "train loss:0.0874188299637374\n",
      "train loss:0.031870260645215205\n",
      "train loss:0.03728711975707954\n",
      "train loss:0.04278260612804642\n",
      "train loss:0.03343853973613069\n",
      "train loss:0.02576676770017998\n",
      "train loss:0.04551185663977174\n",
      "train loss:0.045735003567115094\n",
      "train loss:0.029435907564312885\n",
      "train loss:0.023199187864687543\n",
      "train loss:0.03193031686360774\n",
      "train loss:0.040239464887417246\n",
      "train loss:0.0282329586052162\n",
      "train loss:0.047219647817534474\n",
      "=== epoch:16, train acc:0.984, test acc:0.958 ===\n",
      "train loss:0.06282979697597939\n",
      "train loss:0.026335824599687238\n",
      "train loss:0.09193259965683048\n",
      "train loss:0.061288765201927255\n",
      "train loss:0.03435585345123075\n",
      "train loss:0.054207465638251316\n",
      "train loss:0.03995793974892571\n",
      "train loss:0.02793888197079489\n",
      "train loss:0.04324783684747972\n",
      "train loss:0.037099503245758544\n",
      "train loss:0.06663333406132592\n",
      "train loss:0.07259583931529105\n",
      "train loss:0.028645794424057725\n",
      "train loss:0.02754991370763684\n",
      "train loss:0.048926888934533425\n",
      "train loss:0.05083018904857868\n",
      "train loss:0.03473154789289259\n",
      "train loss:0.04543305849731996\n",
      "train loss:0.015007645760661104\n",
      "train loss:0.038407160721090045\n",
      "train loss:0.05670928037613954\n",
      "train loss:0.06433723357934361\n",
      "train loss:0.029465908225430573\n",
      "train loss:0.023437380181923283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04467061842495511\n",
      "train loss:0.07097492398373664\n",
      "train loss:0.05230099932298961\n",
      "train loss:0.014834095259596674\n",
      "train loss:0.026517188792140246\n",
      "train loss:0.016301072504478008\n",
      "train loss:0.025055219816674676\n",
      "train loss:0.04206502322818272\n",
      "train loss:0.031558584115593334\n",
      "train loss:0.04941747691692266\n",
      "train loss:0.022616085276146832\n",
      "train loss:0.05840165953152053\n",
      "train loss:0.014582359958420723\n",
      "train loss:0.04235786884699186\n",
      "train loss:0.09008515383684135\n",
      "train loss:0.04189459109570266\n",
      "train loss:0.051675632479397965\n",
      "train loss:0.06152933225505046\n",
      "train loss:0.0181250177535062\n",
      "train loss:0.07707658272410228\n",
      "train loss:0.0776160015125807\n",
      "train loss:0.04023666029237476\n",
      "train loss:0.06222192337101588\n",
      "train loss:0.04378130253913311\n",
      "train loss:0.07542792125682453\n",
      "train loss:0.048419539569731614\n",
      "=== epoch:17, train acc:0.986, test acc:0.955 ===\n",
      "train loss:0.015486227346885047\n",
      "train loss:0.03471530888672902\n",
      "train loss:0.027569747218207232\n",
      "train loss:0.0230129145003793\n",
      "train loss:0.043476000062385625\n",
      "train loss:0.01606642727524883\n",
      "train loss:0.02576728164383394\n",
      "train loss:0.04344118111553572\n",
      "train loss:0.05995346494402147\n",
      "train loss:0.038022139782959354\n",
      "train loss:0.03845864933457428\n",
      "train loss:0.09100067431374032\n",
      "train loss:0.023036607551710615\n",
      "train loss:0.025306495193415376\n",
      "train loss:0.07245443946913598\n",
      "train loss:0.018625917921011113\n",
      "train loss:0.013567200996060933\n",
      "train loss:0.03031300541880841\n",
      "train loss:0.018985458503168396\n",
      "train loss:0.02630566130752215\n",
      "train loss:0.04549545073669526\n",
      "train loss:0.04682995259619994\n",
      "train loss:0.029106071882400504\n",
      "train loss:0.01659466823859102\n",
      "train loss:0.031714348236707955\n",
      "train loss:0.04807613243986558\n",
      "train loss:0.014534264500673516\n",
      "train loss:0.0244143959885592\n",
      "train loss:0.04927275873341021\n",
      "train loss:0.016180446658904314\n",
      "train loss:0.022052338892220393\n",
      "train loss:0.014941765256998543\n",
      "train loss:0.03829406227229134\n",
      "train loss:0.03482838295125753\n",
      "train loss:0.02430551112810431\n",
      "train loss:0.023465661423554093\n",
      "train loss:0.02653021928722182\n",
      "train loss:0.024970439251148547\n",
      "train loss:0.029196061656695794\n",
      "train loss:0.01582235672581466\n",
      "train loss:0.031663843331179\n",
      "train loss:0.0490899762368052\n",
      "train loss:0.026665964791676447\n",
      "train loss:0.02675537305694871\n",
      "train loss:0.036435324993286736\n",
      "train loss:0.020465160062533273\n",
      "train loss:0.026774798125319375\n",
      "train loss:0.018921187146566817\n",
      "train loss:0.022647148639236874\n",
      "train loss:0.009974463102724436\n",
      "=== epoch:18, train acc:0.988, test acc:0.962 ===\n",
      "train loss:0.029838154338147585\n",
      "train loss:0.0523065612453355\n",
      "train loss:0.04979757518440885\n",
      "train loss:0.017274039691296185\n",
      "train loss:0.07480787069147594\n",
      "train loss:0.02549579877157036\n",
      "train loss:0.033904263674838606\n",
      "train loss:0.034461076906876054\n",
      "train loss:0.025461909883319662\n",
      "train loss:0.016384493267166414\n",
      "train loss:0.031086228394745424\n",
      "train loss:0.03497180314807367\n",
      "train loss:0.04202669046692489\n",
      "train loss:0.09520272104736063\n",
      "train loss:0.026821810077522653\n",
      "train loss:0.028425644343023505\n",
      "train loss:0.016205560300184968\n",
      "train loss:0.041082106186916774\n",
      "train loss:0.017380463011534356\n",
      "train loss:0.07946160866717453\n",
      "train loss:0.01750906059596317\n",
      "train loss:0.011941258047133836\n",
      "train loss:0.05811427093756528\n",
      "train loss:0.01490509252848237\n",
      "train loss:0.11315438383235772\n",
      "train loss:0.02050566427494501\n",
      "train loss:0.03602133655887173\n",
      "train loss:0.037862519347006504\n",
      "train loss:0.02039130665701277\n",
      "train loss:0.042903974136386115\n",
      "train loss:0.014053404536345262\n",
      "train loss:0.01659680199688034\n",
      "train loss:0.02116908146949714\n",
      "train loss:0.027860708637005627\n",
      "train loss:0.01983321663529771\n",
      "train loss:0.027308156723269302\n",
      "train loss:0.06595348358323569\n",
      "train loss:0.0348342373327271\n",
      "train loss:0.017379228039612057\n",
      "train loss:0.021419494484598112\n",
      "train loss:0.01805888769824016\n",
      "train loss:0.007619773341895919\n",
      "train loss:0.02663588321534197\n",
      "train loss:0.00978949745102677\n",
      "train loss:0.025050517597237608\n",
      "train loss:0.02833166278143985\n",
      "train loss:0.00612708304093039\n",
      "train loss:0.031607063001127075\n",
      "train loss:0.009833100821455616\n",
      "train loss:0.039105523539822024\n",
      "=== epoch:19, train acc:0.992, test acc:0.963 ===\n",
      "train loss:0.02304256951410398\n",
      "train loss:0.00673924825815309\n",
      "train loss:0.028127905109256283\n",
      "train loss:0.06688438124394341\n",
      "train loss:0.01899156597729266\n",
      "train loss:0.06729228186622904\n",
      "train loss:0.07382260216015449\n",
      "train loss:0.022342038369797007\n",
      "train loss:0.02130798476895891\n",
      "train loss:0.039013624533455304\n",
      "train loss:0.017005457721356778\n",
      "train loss:0.010211804934634923\n",
      "train loss:0.015747649144832024\n",
      "train loss:0.023190646866520517\n",
      "train loss:0.02827654859872451\n",
      "train loss:0.008080463212531282\n",
      "train loss:0.05038020203306082\n",
      "train loss:0.009908706488896244\n",
      "train loss:0.022114017302632346\n",
      "train loss:0.05243588673457473\n",
      "train loss:0.010160226955923932\n",
      "train loss:0.03795824749501504\n",
      "train loss:0.049430759605948546\n",
      "train loss:0.030629923548284775\n",
      "train loss:0.00707306943360061\n",
      "train loss:0.01220625488269233\n",
      "train loss:0.013515666131621742\n",
      "train loss:0.015153596849407695\n",
      "train loss:0.034487642946467785\n",
      "train loss:0.026063754669020996\n",
      "train loss:0.02724304137381786\n",
      "train loss:0.024107668302704187\n",
      "train loss:0.04274666310957223\n",
      "train loss:0.013793705288975682\n",
      "train loss:0.029335648003818795\n",
      "train loss:0.009576132617807225\n",
      "train loss:0.018428136866840514\n",
      "train loss:0.014805221251220085\n",
      "train loss:0.010232361487442802\n",
      "train loss:0.004810612511532282\n",
      "train loss:0.013135840955143514\n",
      "train loss:0.02692171012571068\n",
      "train loss:0.013453053775714638\n",
      "train loss:0.014060413605991029\n",
      "train loss:0.019948472709197404\n",
      "train loss:0.015050059046052576\n",
      "train loss:0.011273456965722907\n",
      "train loss:0.03237572517549728\n",
      "train loss:0.0074317995286867725\n",
      "train loss:0.01035604764693563\n",
      "=== epoch:20, train acc:0.992, test acc:0.96 ===\n",
      "train loss:0.02028288995388713\n",
      "train loss:0.02301145890895927\n",
      "train loss:0.013339373035011976\n",
      "train loss:0.029412824393619096\n",
      "train loss:0.03342051970954253\n",
      "train loss:0.017687858775552023\n",
      "train loss:0.018831155164787856\n",
      "train loss:0.0327429350966913\n",
      "train loss:0.00830294338551123\n",
      "train loss:0.03950463513068596\n",
      "train loss:0.00936376306184742\n",
      "train loss:0.03795176981427972\n",
      "train loss:0.02714132936429682\n",
      "train loss:0.011485437569814419\n",
      "train loss:0.009526149264328224\n",
      "train loss:0.032699620407687424\n",
      "train loss:0.006018045780467085\n",
      "train loss:0.02329075173344074\n",
      "train loss:0.023414945284306696\n",
      "train loss:0.010820929018885785\n",
      "train loss:0.014719465549765916\n",
      "train loss:0.02765267873274227\n",
      "train loss:0.02175389526287988\n",
      "train loss:0.01587365462136569\n",
      "train loss:0.020444813856519976\n",
      "train loss:0.03199878283012448\n",
      "train loss:0.011650660480786173\n",
      "train loss:0.016214991778756814\n",
      "train loss:0.03910276588060456\n",
      "train loss:0.016736613301029567\n",
      "train loss:0.019138090912377545\n",
      "train loss:0.009987111584983324\n",
      "train loss:0.0030341170428673354\n",
      "train loss:0.00975521624669709\n",
      "train loss:0.0195886597588073\n",
      "train loss:0.007142052079462194\n",
      "train loss:0.031566563127925946\n",
      "train loss:0.01771799267016266\n",
      "train loss:0.008056665631558036\n",
      "train loss:0.0177981097243\n",
      "train loss:0.010554723903684959\n",
      "train loss:0.01680242972915288\n",
      "train loss:0.007446517280046835\n",
      "train loss:0.026680851365053618\n",
      "train loss:0.01717671437065174\n",
      "train loss:0.015420954352350595\n",
      "train loss:0.026621725236175533\n",
      "train loss:0.03750761407937945\n",
      "train loss:0.010988573829449608\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.959\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.pardir) \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "            filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78975e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
